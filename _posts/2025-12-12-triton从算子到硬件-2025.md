---
layout:     post
title:      triton从算子到编译
subtitle:   triton浅析
date:       2025-12-12
author:     tangcong
header-img: img/triton.png
catalog: true
tags:
    - triton
---

# 1. triton是什么

triton是Open AI推出的，可以以python方式编写高效算子的编程语言。其主要是为了解决使用cuda手写kernel较为繁杂且入门门槛高的问题，使得可以使用python的方式编写可以运行在GPU（非仅NVIDIA）上的kernel，且性能可用。

# 2. 如何编写triton

在使用cuda编写kernel的时候我们考虑的编程模式是grid->block->warp->thread四层，而在triton中，我们考虑的是grid->block两层，也就是说triton编程中我们主要考虑的是grid中各个block如何运行，不关系warp以及thread级别如何运行（即各个SM如何运行而不关注单个core的运行）

在triton中一个block叫做program，以vector_add为例，一个triton kernel如下

```python
import torch

import triton
import triton.language as tl

@triton.jit
def add_kernel(x_ptr,  # 第一个输入向量的指针。
               y_ptr,  # 第二个输入向量的指针。
               output_ptr,  # 输出向量的指针。
               n_elements,  # 向量的大小。
               BLOCK_SIZE: tl.constexpr,  # 每个程序应该处理的元素数量。
               # 注意：`constexpr` 可以作为形状值使用。
               ):
    # 有多个'程序'处理不同的数据。我们在这里标识我们是哪个程序：
    pid = tl.program_id(axis=0)  # 我们使用 1D launch 网格，因此 axis 是 0。
    block_start = pid * BLOCK_SIZE
    # 从这里开始和传统的cuda kernel不同
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # 创建一个掩码以防止内存操作超出范围。
    mask = offsets < n_elements
    # 从 DRAM 加载 x 和 y，以掩盖掉输入不是块大小的倍数的任何额外元素。
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    # 将 x + y 写回到 DRAM。
    tl.store(output_ptr + offsets, output, mask=mask)
```

如果是在传统的cuda kernel中，算出当前thread所在block的偏移后应该计算当前thread的偏移，然后执行当前thread负责的元素的计算，而在triton中，计算出当前block的偏移后，直接使用arrange算出当前block的所有thread的偏移然后执行计算，这样相当于计算粒度是block而不是thread。

# 3. triton的编译

Triton的编译过程是从源语言生成 AST（Abstract Syntax Tree，抽象语法树），借助 dialect遍历 AST，产生 MLIR 表达式（此处可为多层IR通过 Lowering Pass 依次进行分析），最后经过 MLIR 分析器，生成目标硬件程序。

dialect 可以理解为"方言"，各个Dialect分别对不同的层级概念进行建模。比如LLVM Dialect负责系统级别的转换，Linalg，Tensor，Vector等Dialect负责协同生成代码，而Affine，Math等Dialect用来描述底层计算。dialect相当于对特定层级特定领域概念的集合的抽象。

triton的编译分成五个主要阶段：**make_backend、add_stage、load_dialects、make_ir、compile_ir**

## 3.1 make_backend & add_stage

make_backend阶段我们会通过指定的Target名字（例如“cuda”）获取一个CUDABackend，add_stages会增加5个stage，对应编译到不同的IR，如下：

```python
def add_stages(self, stages, options):
        stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)
        stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, self.capability)
        stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options, self.capability)
        stages["ptx"] = lambda src, metadata: self.make_ptx(src, metadata, options, self.capability)
        stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.capability)
```

前面三个stage会经过若干个pass对IR进行分析或优化，后面两个阶段会借助llvm以及ptxas。这些阶段在最后compile_ir步骤实际运行

## 3.2 load_dialects

分为两个步骤：ir.load_dialects和backend.load_dialects

ir.load_dialects主要加载triton的公共dialect和在llvm/mlir中的辅助dialect，例如kernel中的加法操作终最会被ArithDialect的addi这个op表示，而这个op定义在llvm项目里的mlir/include/mlir/Dialect/Arith/IR/ArithBase.td

backend.load_dialects主要加载和backend相关的一些dialects，例如NVIDIA相关的一些dialect，在编译到更低层次的ir时，会用到这些dialects。

## 3.3 make_ir

在将需要的dialect load到上下文管理器之后，我们会开始创建一个初始ir

make_ir的过程是make_ir->ast_to_ttir->ast.parse->generator.visit，在这一过程中ast.parse会把源代码解析成抽象语法树(AST)，其中树的节点包含了源代码的各种抽象模块，比如module、function、args、attr、name、op等等。而在generator.visit阶段，会对这些节点进行遍历lower。

源代码中的"+"操作，先解析为ast的节点ast.BinOp(op.name="\__add__")，进而利用dialect表示为arith::AddIOp

这一步之后得到的ir叫做ttir，对应add_stage中的第一个stage。

## 3.4 compile_ir

Pass（编译过程/变换） 是对 IR（中间表示）进行分析、优化或转换的核心机制

从add_stage可以看出，这一步分为5个小阶段：make_ttir、make_ttgir、make_llir、make_ptx和make_cubin，后两个阶段由llvm和ptxas完成，triton主要做的前三个阶段，这些阶段由各种pass组成

make_ttir: 优化pass

make_ttgir：转换pass，将ttir转换为ttgir（本质是dialect节点的转换）；检查pass，检查op的合法性，如果合法则不用处理，非法的话对其进行转换，这一步有少许硬件相关的pass

make_llir：转换pass，分为两小步，TritonGPU -> LLVM-IR (MLIR) 和 LLVM-IR (MLIR) -> LLVM-IR (LLVM)。第一步是MLIR级别的，也就是在dialect空间的转换，转换的结果就是LLVMDialect，而第二步是将LLVMDialect转换为真正的LLVM IR，此后就可以通过LLVM和ptxas生成可以在GPU上运行的PTX代码了

**参考：**

+ [窥探Triton的lower](https://zhuanlan.zhihu.com/p/695171704)
+ [OpenAI Triton 入门教程](https://zhuanlan.zhihu.com/p/684473453)
+  [Triton极简入门](https://zhuanlan.zhihu.com/p/1902778199261291694)