---
layout:     post
title:      sglang中的lora使用
subtitle:   浅析lora
date:       2025-12-05
author:     tangcong
header-img: img/sglang.jpg
catalog: true
tags:
    - sglang
---

# 1.在tokenizermanager中对lora的处理

在类TokenizerManager中的方法generate_request，会以读锁的方式检查是否启用lora，然后根据lora_path获取lora适配器的id（acquire会增加计数）并赋给obj。然后把请求发送给scheduler，然后进入_wait_one_response函数，在这个函数里面等待请求完成并release掉使用lora适配器，计数减1

同时tokenizermanager也负责lora的动态加载与卸载，因为sglang中可能同时存在多个lora适配器，但不是每个适配器都在使用中或内存中，需要对适配器进行动态的记载和卸载。

加载适配器的入口在类TokenizerCommunicatorMixin的方法load_lora_adapter中，检查是否开启lora和当前lora最大值是否大于lora数量，然后创建一个loraref，通知后端加载适配器，将loraref注册到loraregister中

详细说下后端加载适配器这个过程：在tokenizermanager中将的适配器包装为一个LoadLoRAAdapterReqInput，然后通过commuiter发送给scheduler，scheduler路由到它的load_lora_adapter函数，继续到tpwork的load_lora_adapter，继续到model_runner的load_lora_adapter，然后到了lora_manager.load_lora_adapter，加载到内存中

## 1.1LoRAConfig

对于lora微调，主要公式如下
$$
\Delta W = (B A) \times \frac{\alpha}{r}
$$
主要有三个参数需要确定：α，r，target_modules(应用到哪些模块)

在使用sglang时，我们可以通过--lora-paths参数指定一个或多个LoRA适配器路径（支持Hugging Face模型ID或本地路径），这些lora适配器的关键配置以loraconfig形式存在

## 1.2 LoRARef

可能存在相同的lora_path，为了区分不同的lora适配器，使用loraref类为每一个类绑定一个唯一的id，`--lora-paths` 参数会被转换为 `LoRARef` 对象列表

## 1.3 LoRARegistry

LoRARegistry是一个中央注册表，负责管理所有的lora适配器，所谓的管理，是通过一个lora适配器的name到id的映射表和一个id到引用计数的映射表完成的，这个类在TokenizerManager初始化的时候作为其一个成员初始化

## 1.4 LoRAManager

loramanager管理lora适配器，尤其是适配器在内存中的加载与卸载

## 1.5 LoRAAdapter

loraadapter相当于是一个已经加载到内存中的lora适配器，`LoRAAdapter` 在 `LoRAManager` 中被创建和管理

**LoRAMemoryPool** 从 `LoRAAdapter` 读取权重并加载到 GPU

## 1.6 LoRAMemoryPool

LoRAMemoryPool管理lora适配的内存，首先初始化的时候分配 	`init_buffers()` 方法在 GPU 上预分配所有目标模块的 A 和 B 矩阵缓冲区

`prepare_lora_batch()` 是最关键的方法，在每个推理批次前调用： 

该方法的工作流程：

1. **查找可用槽位**：优先使用空槽位，如果内存池满则使用驱逐策略选择受害者
2. **跳过固定适配器**：不会驱逐 pinned 适配器或当前批次需要的适配器
3. **加载权重**：调用 `load_lora_weight_to_buffer()` 将适配器权重从 CPU 复制到 
4. **更新映射**：维护 UID 和缓冲区 ID 之间的双向映射

# 2.在scheduler中对lora的处理

在 Scheduler 的事件循环中，`recv_requests()` 方法从 ZMQ socket 接收请求，接收到的请求通过 `process_input_requests()` 处理，内部调用分发器路由到load_lora_adapter函数，这个函数也就是tokenizer中发送给scheduler中的函数，将lora适配器加载到GPU中。

scheduler运行完load_lora_adapter后，继续事件循环，假设接受到一个batch的推理请求，处理请求后从batchs中拿出一个batch进行处理，运行get_next_batch_to_run，然后运行run_batch

在run_batch中调用get_model_worker_batch将ScheduleBatch转为ModelWorkerBatch，在这个函数中会收集批次中所有请求的`lora_id`，形成LoRA ID列表

转换完成后执行tp_worker的forward_batch_generation进行推理

当TpModelWorker接收到推理请求时，首先通过`ForwardBatch.init_new()`创建前向传播批次对象。这个过程中，如果启用了LoRA(`enable_lora`)，会自动调用`LoRAManager.prepare_lora_batch()`来准备LoRA权重。

关键设计在于**ForwardBatch**数据结构，它包含了`lora_ids`字段，用于标识批次中每个请求需要使用的LoRA适配器。在`model_runner.forward()`执行前，系统会检查这些LoRA ID，如果某个适配器还未在GPU内存池中，就会从CPU内存动态加载。这种按需加载避免了不必要的GPU内存占用，实现了高效的LoRA管理。

通过ForwardBatch.init_new准备了lora适配器同时得到一个forwar_batch对象存储了lora信息，执行model_runner.forward时把forward_batch作为参数传入

到了model_runner的forward执行_forward_raw选择执行forward，model_runner在初始化的时候通过load_model()加载了model，然后通过model.forward执行实际的推理操作，这就是模型实际根据自己的结构执行操作了

模型操作通过不同的layer进行推理，而如果应用了lora，在ModelRunner初始化的时候会调用init_lora_manager初始化LoRAManager，loramanager初始化调用init_state，在里面调用init_lora_modules，在里面调用set_lora_module，

在set_lora_module中，首先调用get_lora_layer，使用现在的BaseLayerWithLoRA替换之前的layer，然后使用replace_submodule在模型内部进行替换，这样实际推理的时候使用的就是BaseLayerWithLoRA

## 2.1 请求分发器TypeBasedDispatcher

根据接收到的请求对象的**类型**自动路由到相应的处理方法，在 Scheduler 的事件循环中，`recv_requests()` 方法从 ZMQ socket 接收请求，接收到的请求通过 `process_input_requests()` 处理，内部调用分发器。分发器根据请求对象的类型调用相应的处理方法

## 2.2 BaseLayerWithLoRA

`BaseLayerWithLoRA` 是所有 LoRA 包装层的基类。

### 核心属性

- `base_layer`: 原始线性层
- `set_lora`: 是否启用 LoRA（默认 False）
- `lora_backend`: LoRA 计算后端 

### 主要方法

**`forward(x)`** - 前向传播，基类只调用基础层，子类需重写添加 LoRA 计算

**`set_lora_info(\*args)`** - 接收 GPU 内存池的 A/B 矩阵缓冲区，子类需重写

**`slice_lora_a_weights(A, tp_rank)`** - 张量并行时切片 A 矩阵 

**`slice_lora_b_weights(B, tp_rank)`** - 张量并行时切片 B 矩阵

## 2.3 ColumnParallelLinearWithLoRA

### BaseLayerWithLoRA 子类对比表

| 子类名称                               | 基础层类型                   | 主要用途       | A 矩阵切片     | B 矩阵切片                | 通信操作           | 典型应用场景                                     |
| -------------------------------------- | ---------------------------- | -------------- | -------------- | ------------------------- | ------------------ | ------------------------------------------------ |
| **VocabParallelEmbeddingWithLoRA**     | `VocabParallelEmbedding`     | 词嵌入层       | 不适用         | 不适用                    | 不适用             | 目前未实现 LoRA 功能 [1](#19-0)                  |
| **ColumnParallelLinearWithLoRA**       | `ColumnParallelLinear`       | 列并行线性层   | 不切片（完整） | 按输出维度切片            | All-Gather（可选） | 通用列并行投影 [2](#19-1)                        |
| **MergedColumnParallelLinearWithLoRA** | `MergedColumnParallelLinear` | 合并的列并行层 | 不切片（完整） | 按输出维度切片（gate+up） | All-Gather（可选） | MLP 的 gate_up_proj [3](#19-2)                   |
| **QKVParallelLinearWithLoRA**          | `QKVParallelLinear`          | QKV 投影层     | 不切片（完整） | 按 Q/K/V 分别切片         | All-Gather（可选） | Attention 的 QKV 投影 [4](#19-3)                 |
| **RowParallelLinearWithLoRA**          | `RowParallelLinear`          | 行并行线性层   | 按输入维度切片 | 不切片（完整）            | All-Reduce（必须） | Attention 的 o_proj、MLP 的 down_proj [5](#19-4) |

这些方法的forward执行的是基础的计算，apply_lora执行的是增量lora计算，在其中与base_output相加

# 3.在detokenizermanager中对lora的处理

这一阶段与lora无关